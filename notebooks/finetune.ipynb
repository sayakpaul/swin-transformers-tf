{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/swin-transformers-tf/blob/main/notebooks/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89B27-TGiDNB"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u3d4Z7uQsmp"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from:\n",
    "# https://weepingfish.github.io/2020/07/22/0722-suppress-tensorflow-warnings/\n",
    "\n",
    "# Filter tensorflow version warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # or any {'0', '1', '2'}\n",
    "import warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=Warning)\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "tf.autograph.set_verbosity(0)\n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPo10cahZXXQ"
   },
   "source": [
    "## GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpvUOuC3j27n"
   },
   "outputs": [],
   "source": [
    "try:  # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:  # detect GPUs\n",
    "    tpu = False\n",
    "    strategy = (\n",
    "        tf.distribute.get_strategy()\n",
    "    )  # default strategy that works on CPU and single GPU\n",
    "print(\"Number of Accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9S3uKC_iXY5"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Find the list of all fine-tunable models [here](https://tfhub.dev/sayakpaul/collections/swin/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCc6tdUGnD4C"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "IMAGE_SIZE = [224, 224] # Change this accordingly. \n",
    "MODEL_PATH = \"gs://swin-tf/swin_tiny_patch4_window7_224_fe\" \n",
    "\n",
    "# TPU\n",
    "if tpu:\n",
    "    BATCH_SIZE = (\n",
    "        16 * strategy.num_replicas_in_sync\n",
    "    )  # a TPU has 8 cores so this will be 128\n",
    "else:\n",
    "    BATCH_SIZE = 128  # on Colab/GPU, a higher batch size may throw OOM\n",
    "\n",
    "# Dataset\n",
    "CLASSES = [\n",
    "    \"dandelion\",\n",
    "    \"daisy\",\n",
    "    \"tulips\",\n",
    "    \"sunflowers\",\n",
    "    \"roses\",\n",
    "]  # don't change the order\n",
    "\n",
    "# Other constants\n",
    "MEAN = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # imagenet mean\n",
    "STD = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # imagenet std\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iTImGI5qMQT"
   },
   "source": [
    "# Data Pipeline\n",
    "\n",
    "[DeiT authors](https://arxiv.org/abs/2012.12877) use a separate preprocessing pipeline for fine-tuning. But for keeping this walkthrough short and simple, we can just perform the basic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h29TLx7gqN_7"
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = IMAGE_SIZE):\n",
    "    def preprocess(image, label):\n",
    "        # for training, do augmentation\n",
    "        if train:\n",
    "            if tf.random.uniform(shape=[]) > 0.5:\n",
    "                image = tf.image.flip_left_right(image)\n",
    "        image = tf.image.resize(image, size=image_size, method=\"bicubic\")\n",
    "        image = (image - MEAN) / STD  # normalization\n",
    "        return image, label\n",
    "\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
    "\n",
    "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMQ3Qs9_pddU"
   },
   "source": [
    "# Flower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3G-2aUBQJ-H"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
    "    as_supervised=True,\n",
    "    try_gcs=False,  # gcs_path is necessary for tpu,\n",
    ")\n",
    "\n",
    "num_train = tf.data.experimental.cardinality(train_dataset)\n",
    "num_val = tf.data.experimental.cardinality(val_dataset)\n",
    "print(f\"Number of training examples: {num_train}\")\n",
    "print(f\"Number of validation examples: {num_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2X7sE3oRLXN"
   },
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oftrfYw1qXei"
   },
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_dataset, True)\n",
    "val_dataset = make_dataset(val_dataset, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNyCCM6PRM8I"
   },
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaGzFUUVqjaC"
   },
   "outputs": [],
   "source": [
    "sample_images, sample_labels = next(iter(train_dataset))\n",
    "\n",
    "plt.figure(figsize=(5 * 3, 3 * 3))\n",
    "for n in range(15):\n",
    "    ax = plt.subplot(3, 5, n + 1)\n",
    "    image = (sample_images[n] * STD + MEAN).numpy()\n",
    "    image = (image - image.min()) / (\n",
    "        image.max() - image.min()\n",
    "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
    "    plt.imshow(image)\n",
    "    plt.title(CLASSES[sample_labels[n]])\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf6u_7tt8BYy"
   },
   "source": [
    "# LR Scheduler Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVTbnkJL79T_"
   },
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2\n",
    "\n",
    "\n",
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "        learning_rate = (\n",
    "            0.5\n",
    "            * self.learning_rate_base\n",
    "            * (\n",
    "                1\n",
    "                + tf.cos(\n",
    "                    self.pi\n",
    "                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "                    / float(self.total_steps - self.warmup_steps)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALtRUlxhw8Vt"
   },
   "source": [
    "# Model Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JD9SI_Q9JdAB"
   },
   "outputs": [],
   "source": [
    "def get_model(model_url: str, res: int = IMAGE_SIZE[0], num_classes: int = 5) -> tf.keras.Model:\n",
    "    inputs = tf.keras.Input((res, res, 3))\n",
    "    hub_module = hub.KerasLayer(model_url, trainable=True)\n",
    "\n",
    "    x = hub_module(inputs, training=False) \n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpZApp9u9_Y-"
   },
   "outputs": [],
   "source": [
    "get_model(MODEL_PATH).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMfenMQcxAAb"
   },
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D7Iu7oD8WzX"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "WARMUP_STEPS = 10\n",
    "INIT_LR = 0.03\n",
    "WAMRUP_LR = 0.006\n",
    "\n",
    "TOTAL_STEPS = int((num_train / BATCH_SIZE) * EPOCHS)\n",
    "\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=INIT_LR,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    warmup_learning_rate=WAMRUP_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-ID7vP5mIKs"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(scheduled_lrs)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9p4ymNh9y7d"
   },
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnZTSd8K90Mq"
   },
   "outputs": [],
   "source": [
    "with strategy.scope(): # this line is all that is needed to run on TPU (or multi-GPU, ...)\n",
    "    model = get_model(MODEL_PATH)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc7LMVz5Cbx6"
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(history.history)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "result[[\"accuracy\", \"val_accuracy\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[0])\n",
    "result[[\"loss\", \"val_loss\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKFMWzh0Yxsq"
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMEsR851VDZb"
   },
   "outputs": [],
   "source": [
    "sample_images, sample_labels = next(iter(val_dataset))\n",
    "\n",
    "predictions = model.predict(sample_images, batch_size=16).argmax(axis=-1)\n",
    "evaluations = model.evaluate(sample_images, sample_labels, batch_size=16)\n",
    "\n",
    "print(\"[val_loss, val_acc]\", evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzCCDL1CZFx6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5 * 3, 3 * 3))\n",
    "for n in range(15):\n",
    "    ax = plt.subplot(3, 5, n + 1)\n",
    "    image = (sample_images[n] * STD + MEAN).numpy()\n",
    "    image = (image - image.min()) / (\n",
    "        image.max() - image.min()\n",
    "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
    "    plt.imshow(image)\n",
    "    target = CLASSES[sample_labels[n]]\n",
    "    pred = CLASSES[predictions[n]]\n",
    "    plt.title(\"{} ({})\".format(target, pred))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e5oy9zmNNID"
   },
   "source": [
    "# Reference\n",
    "* [Keras Flowers on TPU (solution)](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_solution.ipynb)\n",
    "\n",
    "\n",
    "This notebook is copied and modified from [here](https://github.com/sayakpaul/ConvNeXt-TF/blob/main/notebooks/finetune.ipynb). I'm thankful to [awsaf49](https://github.com/awsaf49) who originally worked on that notebook. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "finetune",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
